import json
import logging
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import HashingVectorizer
from vanna.utils import deterministic_uuid

logger = logging.getLogger(__name__)


class ThriveAI_Milvus:
    """Milvus Lite-backed vector store with dense+sparse fields and role-aware metadata.

    Notes
    - Uses Milvus Lite (in-process) when configured with mode == "lite".
    - Schema fields (per collection):
      id (VARCHAR, PK), text (VARCHAR, analyzer enabled), text_dense (FLOAT_VECTOR), text_sparse (SPARSE_FLOAT_VECTOR), user_role (INT64)
    - Sparse vectors are generated by Milvus BM25 function from the `text` field.
    - Dense vectors are generated via a fast, deterministic HashingVectorizer to a fixed dimension for development.
      Replace with model-based embeddings when integrating with LLM embedding APIs.
    """

    def __init__(self, user_role: int, config: dict | None = None):
        try:
            self.user_role = user_role
            self.config = config.copy() if config else {}
            self._dim = int(self.config.get("text_dim", 768))
            self._collection_prefix = self.config.get("collection_prefix", "thrive")

            # Resolve file-backed Milvus Lite URI and create client (no server startup)
            self._uri = self._resolve_uri()
            self._client = self._get_client()

            # Ensure collections exist
            self.sql_collection = f"{self._collection_prefix}_sql"
            self.ddl_collection = f"{self._collection_prefix}_ddl"
            self.documentation_collection = f"{self._collection_prefix}_docs"

            self._ensure_collection(self.sql_collection)
            self._ensure_collection(self.ddl_collection)
            self._ensure_collection(self.documentation_collection)

            # Deterministic hashing vectorizer for dense embeddings (dev-time)
            # n_features must equal the configured dim
            self._vectorizer = HashingVectorizer(n_features=self._dim, alternate_sign=False, norm="l2")

            # Default retrieval sizes (parity with Chroma = 10), overridable via config
            _top_k = int(self.config.get("top_k", 10))
            self.n_results_sql = int(self.config.get("n_results_sql", _top_k))
            self.n_results_ddl = int(self.config.get("n_results_ddl", _top_k))
            self.n_results_documentation = int(self.config.get("n_results_documentation", _top_k))
        except Exception as e:
            logger.exception("Error initializing ThriveAI_Milvus: %s", e)
            raise

    # ---- Milvus client helpers ----
    def _resolve_uri(self) -> str:
        """Determine the Milvus Lite file path URI.

        Priority:
        - config['uri'] if provided
        - config['persist_path'] if provided
        - default './milvus_demo.db' in project root
        """
        uri = self.config.get("uri") or self.config.get("persist_path")
        if not uri:
            uri = "./milvus_demo.db"
        # Expand and normalize to string path
        return str(Path(uri).expanduser().resolve())

    def _get_client(self):
        try:
            # Use the high-level MilvusClient with a file-backed URI (Milvus Lite)
            from pymilvus import MilvusClient

            return MilvusClient(self._uri)
        except Exception as e:
            logger.exception("Error creating Milvus client: %s", e)
            raise

    def _ensure_collection(self, collection_name: str):
        from pymilvus import DataType, Function, FunctionType, utility

        try:
            if self._client.has_collection(collection_name):
                # Ensure sparse index exists for BM25 searches (idempotent)
                try:
                    self._client.create_index(
                        collection_name=collection_name,
                        field_name="text_sparse",
                        index_params={"index_type": "SPARSE_INVERTED_INDEX"},
                    )
                except Exception:
                    pass
                # Wait for index and loading to complete before proceeding
                try:
                    utility.wait_for_index_building_complete(collection_name)
                except Exception:
                    pass
                # Make sure collection is loaded
                try:
                    self._client.load_collection(collection_name=collection_name)
                except Exception:
                    pass
                return

            schema = self._client.create_schema(auto_id=False)
            schema.add_field(field_name="id", datatype=DataType.VARCHAR, is_primary=True, max_length=64)
            schema.add_field(field_name="text", datatype=DataType.VARCHAR, max_length=1000, enable_analyzer=True)
            schema.add_field(field_name="text_dense", datatype=DataType.FLOAT_VECTOR, dim=self._dim)
            schema.add_field(field_name="text_sparse", datatype=DataType.SPARSE_FLOAT_VECTOR)
            schema.add_field(field_name="user_role", datatype=DataType.INT64)

            bm25_fn = Function(
                name="text_bm25_emb",
                input_field_names=["text"],
                output_field_names=["text_sparse"],
                function_type=FunctionType.BM25,
            )
            schema.add_function(bm25_fn)

            index_params = self._client.prepare_index_params()
            # Milvus Lite supports FLAT, IVF_FLAT, AUTOINDEX. Use IVF_FLAT with IP.
            index_params.add_index(
                field_name="text_dense",
                index_name="text_dense_index",
                index_type="IVF_FLAT",
                metric_type="IP",
                params={"nlist": 128},
            )
            # Create sparse text index if supported by current Milvus build
            try:
                index_params.add_index(
                    field_name="text_sparse",
                    index_name="text_sparse_index",
                    index_type="SPARSE_INVERTED_INDEX",
                    metric_type="BM25",
                    params={"inverted_index_algo": "DAAT_MAXSCORE"},  # or "DAAT_WAND" or "TAAT_NAIVE"
                )
            except Exception:
                pass

            self._client.create_collection(
                collection_name=collection_name,
                schema=schema,
                index_params=index_params,
                consistency_level="Strong",
            )

            # Load into memory for searching
            # Wait for index build and load collection
            try:
                utility.wait_for_index_building_complete(collection_name)
            except Exception:
                pass
            self._client.load_collection(collection_name=collection_name)
        except Exception as e:
            logger.exception("Error ensuring collection %s: %s", collection_name, e)
            raise

    # ---- Metadata helpers ----
    def _is_role_restriction_enabled(self) -> bool:
        """Check secrets for role restriction toggle. Defaults to True when unavailable."""
        try:
            import streamlit as st  # optional in tests

            # Support both top-level and nested under security
            if "restrict_rag_by_role" in st.secrets:
                return bool(st.secrets.get("restrict_rag_by_role"))
            security = st.secrets.get("security", {})
            return bool(security.get("restrict_rag_by_role", True))
        except Exception:
            return True

    def _get_effective_role(self) -> int:
        """Return 0 when restriction disabled to allow all training; else the user role."""
        return 0 if not self._is_role_restriction_enabled() else int(self.user_role)

    def _prepare_metadata(self, metadata: dict[str, Any] | None = None) -> dict[str, Any]:
        if metadata is None:
            metadata = {}
        metadata["user_role"] = self._get_effective_role()
        return metadata

    def _prepare_retrieval_metadata(self, metadata: dict[str, Any] | None = None) -> str:
        """Return a Milvus filter expression enforcing role-based visibility (overridable)."""
        role = self._get_effective_role()
        # Additional AND conditions can be appended if metadata includes more filters
        return f"user_role >= {int(role)}"

    # ---- Embeddings ----
    def generate_embedding(self, text: str) -> list[float]:
        """Generate embedding using Ollama if configured; fallback to hashing."""
        try:
            import streamlit as st  # lazy import to avoid coupling in tests

            ai_keys = st.secrets.get("ai_keys", {})
            embed_model = ai_keys.get("ollama_embed_model")
            if embed_model:
                try:
                    import ollama

                    host = ai_keys.get("ollama_host", "http://localhost:11434")
                    client = ollama.Client(host)
                    res = client.embeddings(model=embed_model, prompt=text)
                    emb = res.get("embedding")
                    if isinstance(emb, list) and emb:
                        # Align to configured dimension
                        if len(emb) > self._dim:
                            emb = emb[: self._dim]
                        elif len(emb) < self._dim:
                            emb = emb + [0.0] * (self._dim - len(emb))
                        return emb
                except Exception:
                    pass
        except Exception:
            # st.secrets may not be available in some test contexts
            pass

        # Fallback: Deterministic hashing vector
        vec = self._vectorizer.transform([text])
        dense = vec.toarray().astype(np.float32)[0]
        return dense.tolist()

    # ---- Write APIs ----
    def add_question_sql(self, question: str, sql: str, metadata: dict[str, Any] | None = None, **kwargs) -> str:
        try:
            question_sql_json = json.dumps({"question": question, "sql": sql}, ensure_ascii=False)
            doc_id = deterministic_uuid(question_sql_json) + "-sql"
            self._insert(self.sql_collection, doc_id, question_sql_json, metadata)
            return doc_id
        except Exception as e:
            logger.exception("Error adding question/sql to Milvus: %s", e)
            raise

    def add_ddl(self, ddl: str, metadata: dict[str, Any] | None = None, **kwargs) -> str:
        try:
            doc_id = deterministic_uuid(ddl) + "-ddl"
            self._insert(self.ddl_collection, doc_id, ddl, metadata)
            return doc_id
        except Exception as e:
            logger.exception("Error adding DDL to Milvus: %s", e)
            raise

    def add_documentation(self, documentation: str, metadata: dict[str, Any] | None = None, **kwargs) -> str:
        try:
            doc_id = deterministic_uuid(documentation) + "-doc"
            self._insert(self.documentation_collection, doc_id, documentation, metadata)
            return doc_id
        except Exception as e:
            logger.exception("Error adding documentation to Milvus: %s", e)
            raise

    def remove_training_data(self, entry_id: str) -> bool:
        """Remove a training entry by ID from all collections.

        Returns True if deletion attempts were issued without fatal errors.
        """
        success = False
        try:
            try:
                self._client.delete(collection_name=self.sql_collection, ids=[entry_id])
                success = True
            except Exception:
                pass
            try:
                self._client.delete(collection_name=self.ddl_collection, ids=[entry_id])
                success = True
            except Exception:
                pass
            try:
                self._client.delete(collection_name=self.documentation_collection, ids=[entry_id])
                success = True
            except Exception:
                pass
        except Exception as e:
            logger.exception("Error removing training data %s: %s", entry_id, e)
        return success

    def _insert(self, collection: str, doc_id: str, text: str, metadata: dict[str, Any] | None):
        try:
            dense = self.generate_embedding(text)
            row = {
                "id": doc_id,
                "text": text,
                "text_dense": dense,
                # text_sparse will be generated by BM25 function from `text`
                "user_role": self._prepare_metadata(metadata)["user_role"],
            }
            self._client.insert(collection_name=collection, data=[row])
        except Exception as e:
            logger.exception("Error inserting into %s: %s", collection, e)
            raise

    # ---- Read APIs ----
    def get_training_data(self, metadata: dict[str, Any] | None = None, **kwargs) -> pd.DataFrame:
        try:
            filter_expr = self._prepare_retrieval_metadata(metadata)
            cols = ["id", "text", "user_role"]

            df = pd.DataFrame()

            for collection, ttype in [
                (self.sql_collection, "sql"),
                (self.ddl_collection, "ddl"),
                (self.documentation_collection, "documentation"),
            ]:
                recs = self._client.query(collection_name=collection, filter=filter_expr, output_fields=cols)
                if recs:
                    tmp = pd.DataFrame(recs)
                    if ttype == "sql":
                        docs = [json.loads(t) for t in tmp["text"].tolist()]
                        out = pd.DataFrame(
                            {
                                "id": tmp["id"].tolist(),
                                "question": [d.get("question") for d in docs],
                                "content": [d.get("sql") for d in docs],
                                "training_data_type": ttype,
                            }
                        )
                    else:
                        out = pd.DataFrame(
                            {
                                "id": tmp["id"].tolist(),
                                "question": [None for _ in range(len(tmp))],
                                "content": tmp["text"].tolist(),
                                "training_data_type": ttype,
                            }
                        )
                    df = pd.concat([df, out], ignore_index=True)
            return df
        except Exception as e:
            logger.exception("Error getting training data from Milvus: %s", e)
            return pd.DataFrame()

    def get_similar_question_sql(self, question: str, metadata: dict[str, Any] | None = None, **kwargs) -> list:
        """Return similar question/SQL pairs using hybrid dense+sparse with RRF.

        Uses the same robust path as documentation/ddl to avoid hybrid_search metric/index issues.
        """
        try:
            texts = self._hybrid_docs(self.sql_collection, question, limit=self.n_results_sql, metadata=metadata)
            out: list[dict] = []
            for t in texts:
                try:
                    out.append(json.loads(t))
                except Exception:
                    continue
            return out
        except Exception as e:
            logger.exception("Error in get_similar_question_sql: %s", e)
            return []

    # ---- Additional retrieval helpers using hybrid search ----
    def _hybrid_docs(
        self, collection_name: str, question: str, limit: int = 5, metadata: dict[str, Any] | None = None
    ) -> list[str]:
        """Hybrid retrieval by combining separate dense and sparse searches with simple RRF."""
        filter_expr = self._prepare_retrieval_metadata(metadata)
        dense_vec = self.generate_embedding(question)

        # Perform dense search
        dense_hits = self._client.search(
            collection_name=collection_name,
            data=[dense_vec],
            anns_field="text_dense",
            limit=limit,
            filter=filter_expr,
            output_fields=["id", "text", "user_role"],
        )

        # Perform sparse (BM25) search using query text
        try:
            try:
                from pymilvus import AnnSearchRequest, RRFRanker

                req_sparse = AnnSearchRequest(
                    data=[question], anns_field="text_sparse", param={"drop_ratio_search": 0.2}, limit=limit
                )
                sparse_hits = self._client.hybrid_search(
                    collection_name=collection_name,
                    reqs=[req_sparse],
                    ranker=RRFRanker(100),
                    limit=limit,
                    filter=filter_expr,
                    output_fields=["id", "text", "user_role"],
                )
            except Exception:
                sparse_hits = self._client.search(
                    collection_name=collection_name,
                    data=[question],
                    anns_field="text_sparse",
                    limit=limit,
                    filter=filter_expr,
                    output_fields=["id", "text", "user_role"],
                )
        except Exception:
            # Fallback: no sparse search available; use only dense results
            sparse_hits = []

        # Combine with simple RRF
        def to_list(hits):
            # milvus client returns a list of lists
            if not hits:
                return []
            return hits[0] if isinstance(hits, list) else []

        dense_list = to_list(dense_hits)
        sparse_list = to_list(sparse_hits)

        rrf_scores: dict[str, float] = {}
        k = 60.0

        for rank, hit in enumerate(dense_list, start=1):
            eid = hit.get("id") or hit.get("entity", {}).get("id") or hit.get("primary_key")
            if eid is None:
                continue
            rrf_scores[eid] = rrf_scores.get(eid, 0.0) + 1.0 / (k + rank)

        for rank, hit in enumerate(sparse_list, start=1):
            eid = hit.get("id") or hit.get("entity", {}).get("id") or hit.get("primary_key")
            if eid is None:
                continue
            rrf_scores[eid] = rrf_scores.get(eid, 0.0) + 1.0 / (k + rank)

        # Retrieve text for top ids in combined ranking
        ranked_ids = [eid for eid, _ in sorted(rrf_scores.items(), key=lambda kv: kv[1], reverse=True)][:limit]

        # Build a quick map from both hit lists
        text_map: dict[str, str] = {}
        for hit in dense_list + sparse_list:
            try:
                eid = hit.get("id") or hit.get("entity", {}).get("id") or hit.get("primary_key")
                txt = hit.get("text") or hit.get("entity", {}).get("text")
                if eid and txt is not None and eid not in text_map:
                    text_map[eid] = txt
            except Exception:
                continue

        docs: list[str] = [text_map[eid] for eid in ranked_ids if eid in text_map]
        return docs

    def get_related_ddl(self, question: str, metadata: dict[str, Any] | None = None, **kwargs) -> list:
        try:
            return self._hybrid_docs(self.ddl_collection, question, limit=self.n_results_ddl, metadata=metadata)
        except Exception as e:
            logger.exception("Error in get_related_ddl: %s", e)
            return []

    def get_related_documentation(self, question: str, metadata: dict[str, Any] | None = None, **kwargs) -> list:
        try:
            return self._hybrid_docs(
                self.documentation_collection, question, limit=self.n_results_documentation, metadata=metadata
            )
        except Exception as e:
            logger.exception("Error in get_related_documentation: %s", e)
            return []

    def hybrid_query_text(
        self, collection_name: str, question: str, top_k: int = 10, metadata: dict[str, Any] | None = None
    ) -> list[str]:
        """Generic hybrid query over a named collection, returns raw text documents."""
        try:
            return self._hybrid_docs(collection_name, question, limit=top_k, metadata=metadata)
        except Exception as e:
            logger.exception("Error in hybrid_query_text: %s", e)
            return []
